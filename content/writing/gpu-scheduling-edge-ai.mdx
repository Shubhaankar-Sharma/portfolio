---
title: "Working on Timing Guarantees for AI in Healthcare"
description: "Research on making AI-assisted surgery predictable and safe, and what I learned about confirmation bias and scientific thinking."
date: "2025-11-08"
tags: ["research", "real-time systems", "healthcare"]
image: "https://cdn.spongeboi.com/holoscan-art.png"
---

import GreenContextSimulation from '../../app/components/GreenContextSimulation'

## Context

<mark>Imagine a surgeon performing a delicate operation wearing AR glasses that overlay AI predicted organ positions onto the live surgical feed. Systems like these could go from assistance to performing minor surgeries in a lot of situations.</mark> <mark>We could deploy such robots in underdeveloped regions with shortage on trained doctors and help a lot of people.</mark> These tools run locally without needing internet. Industrialization of full surgical robots is way out in the future though. Right now assistance tools help in education, preserve context during surgeries, and help doctors make better decisions, for example which maneuvers to perform when.

These systems need to be reliable. A lag of even a few milliseconds during surgery could be catastrophic. My work with Dr. Arpan Gujarati, Philip Schowitz, and others focused on exactly this problem. Applications like [ORSI Multi AI AR](https://nvidia-holoscan.github.io/holohub/applications/orsi/orsi_segmentation_ar/) run on NVIDIA's Holoscan SDK.

I was drawn to this project because it had real impact potential. I sat through so many surgery videos to benchmark these applications.

Fair warning: the following sections get a bit technical.

## The Work

My work spanned two interconnected research directions. This contributed to a paper accepted at RTSS 2025: ["Faster, Exact, More General Response-time Analysis for NVIDIA Holoscan Applications"](https://2025.rtss.org/program/index.html) (Philip Schowitz, Shubhaankar Sharma, Soham Sinha, Bruce Shepherd, Arpan Gujarati).

**Timing Analysis**

I needed to validate the theory with real-world data. This meant benchmarking Holoscan applications on NVIDIA IGX Orin hardware to determine WCRT values (worst-case response time). These benchmarks ran for days.

I developed an artifact that automated WCRT measurements using UPPAAL (a formal verification tool). This got accepted at RTSS 2025. (GitHub Repository: [rt-holoscan-artifacts](https://github.com/ubc-systopia/rt-holoscan-artifacts))

**GPU Scheduling**

The second half of my internship and ongoing work focuses on a different question: could we reduce latency and jitter by changing how the GPU allocates resources? CPUs have 8-16 cores. GPUs have thousands of parallel streaming multiprocessors (SMs). Holoscan has unique queueing behavior we could exploit. We wanted to guarantee the system won't take longer than X milliseconds with minimal variance in output timing.

Tracing operations in a GPU-accelerated pipeline is complex. <mark>The CPU and GPU operate on different clocks. The pipeline can move ahead on the CPU while earlier tasks execute on the GPU, sometimes the CPU is three iterations ahead.</mark> I created a Python script to recreate CPU iterations and their related GPU kernel executions, and understand GPU kernel scheduling time.

After developing a metrics analysis framework, I could quickly analyze proposed policies. However, the first experiment design proved infeasible (exponential runtime scaling). After two months of work, this was disappointing. <mark>I had fallen victim to confirmation bias. When I got results that matched my expectations, I didn't dig deeper to verify them.</mark>

We redesigned the experiment with linear runtime scaling. After several iterations, our policy reduced application jitter, providing more consistent outputs and better P99 latency.

## Interactive Demo

Below is an interactive simulation showing how our Green Context (pGC) policy improves frame delivery consistency compared to the default nGC policy. This is novel work still in progress. The simulation shows two pipeline policies processing events through a 5-stage pipeline.

<GreenContextSimulation />

The default nGC policy creates unbalanced compute allocation. Fast stages process frames quickly while the bottleneck stage creates backpressure. When busy with a queued frame, new frames drop. Result: inconsistent output, dropped frames.

Our pGC policy achieves balanced allocation with consistent processing (~22ms across all stages). No bottlenecks, the queue never overflows. Result: zero frame drops, consistent output.

<mark>The critical insight: by managing GPU streaming multiprocessor allocation, we can make event processing consistent along the pipeline.</mark> The default policy drops frames because unbalanced stages process at mismatched rates, causing queue overflow.

## Reflections

This internship opened doors to a whole new world. <mark>I learnt that research requires strong theoretical foundations. Theory should predict what you expect to see. When reality deviates, that's where discoveries happen.</mark> Before this, I thought I understood the scientific method. I didn't. <mark>Confirmation bias is incredibly powerful. Even when you know about it, you can still fall victim to it.</mark> You see the patterns you want to see. Peer review, reproducibility, and publishing artifacts help mitigate this.

Through this research, I started diving deep into epistemology. How do we know what we know? How does our perception deceive us? <mark>I discovered [Bell Labs](https://ethw.org/Bell_Telephone_Laboratories,_Inc._List_of_Significant_Innovations_%26_Discoveries_(1925-1983)). I learned about the invisible engineers whose work shapes our entire modern world. The transistor. Unix. C. Information theory. All from one lab.</mark> You can have a profound impact by diving deep into specialized research.

What struck me was how deep every field goes. I'm experiencing this at PlanetScale, where I'm working on [neki](https://neki.dev) (a sharded Postgres system). I'm diving into SQL databases: query planning, storage orchestration, it's endless. I'm also working on research around probabilistic concurrency testing. We're still working on deeper investigations into GPU scheduling policies at Systopia. We have one paper published at RTSS 2025.

After seeing the mind blowing work people do, I'm grateful that I found people in my life who have supported me on this journey, and given me opportunities like this that allow me to work on things that truly intrigue me and can also lead to small contributions to science. I hope I can do that throughout my life.
