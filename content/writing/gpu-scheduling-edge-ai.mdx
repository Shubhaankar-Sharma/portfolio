---
title: "My First Research Internship: GPU Scheduling for Real-Time Edge AI"
description: "Reflections on my experience working on real-time systems research, including setbacks, breakthroughs, and lessons learned."
date: "2025-01-15"
tags: ["research", "experience", "real-time systems"]
image: "https://cdn.spongeboi.com/holoscan-art.png"
---

import GreenContextSimulation from '../../app/components/GreenContextSimulation'

## Background

Modern hardware is capable of running powerful AI/ML computations even on smaller form-factor devices. NVIDIA's Jetson and IGX platforms enable sophisticated neural networks to run locally rather than in the cloud.

These edge AI capabilities have tremendous applications in medicine, autonomous vehicles, and robotics. These applications require on-premise processing for privacy and latency. In surgical assistance, systems must respond in real-time and cannot send patient data to cloud servers.

One such application is **ORSI Multi AI AR**, which overlays AI-predicted organ positions onto live endoscopy feeds during surgery. These applications run on edge computers like NVIDIA IGX Orin, built on frameworks such as NVIDIA's Holoscan SDK, which orchestrates real-time data processing through graph-based pipelines.

## The Research Problem

One key challenge here is: **how can we make the performance of NVIDIA GPUs predictable for our edge applications?** This is extremely important for cyber-physical system (CPS) applications, as timing is part of the application's safety specification. In mission-critical applications like surgical assistance software, inconsistent frame delivery or unpredictable latency can affect reliability and safety.

While Holoscan has benchmarks, ensuring predictable real-time behavior requires deeper timing analysis. Dr. Arpan Gujarati's research group previously published work on statically analyzing CPU-based systems, developing an algorithmic approach to determine maximum end-to-end time given worst-case response times (WCRTs) of individual nodes in the application graph. In 2024-2025, this analysis was improved, and got accepted at RTSS 2025 as **["Faster, Exact, More General Response-time Analysis for NVIDIA Holoscan Applications"](https://2025.rtss.org/program/index.html)** (Philip Schowitz, Shubhaankar Sharma, Soham Sinha, Bruce Shepherd, Arpan Gujarati).

![RTSS 2025 Artifact Evaluated Badge](https://cdn.spongeboi.com/Screenshot%202025-10-22%20at%207.30.56%E2%80%AFPM.png)

My work spanned two interconnected research directions: completing the improved timing analysis and investigating GPU partitioning to reuse these analyses with GPUs as the computational unit.

## Problem 1: Timing Analysis & Verification

I had volunteered on this project for three months before the internship, developing foundational knowledge of Holoscan. When I joined as a research intern, the Research Group had already developed the theoretical framework for static timing analysis and were submitting it to RTSS 2025.

My contribution focused on **empirical validation**: benchmarking Holoscan applications to determine WCRT values on NVIDIA IGX Orin. These profiling experiments were extensive, often taking 6+ hours to capture worst-case behavior.

I developed and packaged an artifact that automated WCRT measurements and generated system-level timing analyses using UPPAAL, a formal verification tool. This workflow was submitted and accepted as an artifact to RTSS 2025, enabling other researchers to reproduce our methodology.

**GitHub Repository**: [rt-holoscan-artifacts](https://github.com/ubc-systopia/rt-holoscan-artifacts)

## Problem 2: GPU Scheduling Policy Optimization

The remaining three months of my internship focused on investigating whether better GPU resource allocation could reduce latency and jitter. The problem is analogous to CPU thread allocation but more complex: while CPUs have a handful of cores, **GPUs have thousands of parallel streaming multiprocessors (SMs)**.

We hypothesized that a custom SM allocation policy exploiting Holoscan's unique queueing behavior could reduce jitter and improve predictability.

### The Experimentation Process

I led the experimentation through multiple iterations, writing CUDA kernels, developing Holoscan application code, and creating analysis pipelines using NumPy and Pandas. We used **NSight Systems** to profile GPU and CPU execution.

Tracing operations is nontrivial—the CPU and GPU operate on different clocks with different timescales. The pipeline can move ahead on the CPU while earlier tasks execute on the GPU; the CPU could be three iterations ahead. Understanding which iterations ran when was essential to verify Holoscan's behavior.

I maintained a detailed mental model of the application, which helped me predict GPU scheduling behavior and propose experiment designs. By anticipating how kernels should behave, we identified issues in the expected parallelism behavior.

### The Setback

After developing a metrics analysis framework, I could quickly analyze proposed policies. However, the first experiment design proved infeasible—individual kernels had exponential runtime scaling, limiting potential improvements. **After two months of work, this was disappointing.**

### The Breakthrough

We redesigned the experiment with kernels having linear runtime scaling. After several iterations, we got results matching theoretical expectations. **Our policy reduced application jitter**, providing more consistent outputs and better P99 latency, improving reliability in surgical applications.

## Interactive Demonstration: Green Context Policy

Below is an interactive simulation showing how our <mark>Green Context (pGC) policy improves frame delivery consistency</mark> compared to the default nGC allocation policy.

The simulation demonstrates two pipeline policies processing events (frames) through a 5-stage pipeline:

**Understanding the Visualization:**
- **Events (circles)** represent video frames moving through the pipeline
- **Processing (blue/green circles)** shows active computation
- **Queued (yellow circles)** indicates frames waiting for the next stage
- **Dropped (red boxes)** shows frames that were discarded due to queue overflow
- Each stage can only process one event at a time
- Queue capacity is limited to 1 event per stage

<GreenContextSimulation />

### Why Default Policy (nGC) Drops Frames

The default nGC policy creates an <mark>unbalanced compute allocation</mark>:
- **Fast stages** (Source: 5ms, Middle0: 8ms) process frames quickly
- **Bottleneck stage** (Middle1: 35ms) creates severe backpressure
- **Queue overflow**: When Middle1 is busy and the queue already has 1 waiting frame, new frames are dropped
- **Result**: Inconsistent output, dropped frames, unreliable video feed

### Why Green Context Policy (pGC) Succeeds

Our pGC policy achieves <mark>balanced compute allocation</mark>:
- **Consistent processing** (~22ms across all stages)
- **No bottlenecks**: All stages complete at similar rates
- **Queue never overflows**: Balanced throughput means no backpressure
- **Result**: Zero frame drops, consistent video quality, predictable behavior

### Key Results

Our Green Context policy achieves:
- **Zero frame drops** in balanced workloads
- **Reduced P99 latency** through consistent stage processing times
- **Improved predictability** for real-time guarantees
- **Better resource utilization** by balancing GPU compute allocation

**The critical insight:** <mark>By appropriately managing GPU streaming multiprocessor (SM) allocation, we can make event processing consistent along the pipeline.</mark> With limited cache/queue sizes in Holoscan, the default policy drops frames because unbalanced stages process at mismatched rates, causing queue overflow at bottleneck points.

## Research Experience and Takeaways

The most valuable lesson came from our setback with the first experiment design. When I got results that appeared to match my expectations, I didn't dig deeper to verify them—I had fallen victim to **confirmation bias**. These results turned out to be wrong, and we had spent two months pursuing a dead end.

This taught me that **research requires a strong theoretical foundation before conducting experiments**. Theory should predict what you expect to see, and when reality deviates, that's when the most interesting discoveries happen—but only if you have the theory to compare against.

### Skills Developed

Beyond technical skills in CUDA programming and GPU architecture, I developed crucial research skills:

- Maintaining detailed mental models of complex systems
- Designing experiments to isolate key behaviors
- Presenting complex research in simple terms
- The discipline to question results even when they confirm what I want to believe

## Conclusion

This research demonstrates that thoughtful GPU resource management can significantly improve the reliability of edge AI systems in critical applications. By developing policies that balance compute allocation across pipeline stages, we can reduce frame drops and improve timing predictability—essential properties for surgical assistance and other safety-critical cyber-physical systems.

The accepted RTSS 2025 artifact enables other researchers to build on this work, and our findings on GPU scheduling policies open new directions for optimizing real-time edge AI applications.
